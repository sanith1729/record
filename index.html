<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Screen and Audio Recorder</title>
  <style>
    body {
      font-family: sans-serif;
      padding: 20px;
    }
    video {
      max-width: 100%;
      border: 1px solid #ccc;
      margin-bottom: 10px;
    }
    button {
      margin-right: 5px;
      margin-bottom: 10px;
    }
  </style>
</head>
<body>
  <h1>Screen and Audio Recorder</h1>
  <p>
    <strong>Note:</strong> When prompted, please choose <em>Entire Screen</em> so that all system audio—including speech synthesis—can be captured.
  </p>
  <video id="preview" autoplay muted></video>
  <br />
  <button id="start">Start Recording</button>
  <button id="stop" disabled>Stop Recording</button>
  <button id="speak">Test Speech Synthesis</button>
  <br />
  <a id="download" style="display: none;">Download Recording</a>

  <script>
    let mediaRecorder;
    let recordedChunks = [];

    document.getElementById('start').addEventListener('click', async () => {
      try {
        // Request screen stream (choose Entire Screen to capture system audio)
        const screenStream = await navigator.mediaDevices.getDisplayMedia({
          video: true,
          audio: true
        });

        // Request microphone audio
        const micStream = await navigator.mediaDevices.getUserMedia({
          audio: true
        });

        // Debug logs
        console.log("Screen audio tracks:", screenStream.getAudioTracks());
        console.log("Mic audio tracks:", micStream.getAudioTracks());

        // Create an AudioContext to merge audio sources
        const audioContext = new AudioContext();
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
        }
        const destination = audioContext.createMediaStreamDestination();

        // If screen share has an audio track, connect it
        if (screenStream.getAudioTracks().length > 0) {
          const systemSource = audioContext.createMediaStreamSource(screenStream);
          systemSource.connect(destination);
        } else {
          console.warn("No system audio track available from screen share.");
        }

        // Connect microphone audio if available
        if (micStream.getAudioTracks().length > 0) {
          const micSource = audioContext.createMediaStreamSource(micStream);
          micSource.connect(destination);
        } else {
          console.warn("No microphone audio track available.");
        }

        // Combine the screen's video with the merged audio tracks
        const combinedStream = new MediaStream([
          ...screenStream.getVideoTracks(),
          ...destination.stream.getAudioTracks()
        ]);

        // Show a live preview of the recording
        document.getElementById('preview').srcObject = combinedStream;

        // Set up the MediaRecorder on the combined stream
        mediaRecorder = new MediaRecorder(combinedStream);
        mediaRecorder.ondataavailable = event => {
          if (event.data.size > 0) {
            recordedChunks.push(event.data);
          }
        };

        mediaRecorder.onstop = () => {
          const blob = new Blob(recordedChunks, { type: 'video/webm' });
          const url = URL.createObjectURL(blob);
          const downloadLink = document.getElementById('download');
          downloadLink.href = url;
          downloadLink.download = 'recording.webm';
          downloadLink.style.display = 'inline';
          recordedChunks = [];
        };

        mediaRecorder.start();
        document.getElementById('start').disabled = true;
        document.getElementById('stop').disabled = false;
      } catch (error) {
        console.error('Error accessing media devices:', error);
        alert('Error accessing media devices. Please check console for details.');
      }
    });

    document.getElementById('stop').addEventListener('click', () => {
      if (mediaRecorder) {
        mediaRecorder.stop();
        document.getElementById('start').disabled = false;
        document.getElementById('stop').disabled = true;
      }
    });

    document.getElementById('speak').addEventListener('click', () => {
      // This will trigger speech synthesis.
      const msg = new SpeechSynthesisUtterance('This is a test of speech synthesis audio recording.');
      window.speechSynthesis.speak(msg);
    });
  </script>
</body>
</html>
